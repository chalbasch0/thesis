{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Results in Realtime\n",
    "Use Tweepy's Library. There are ways of storing this data continuously in a SQLite database, or locally on your computer. See https://towardsdatascience.com/mining-live-twitter-data-for-sentiment-analysis-of-events-d69aa2d136a1 for more info on contiunuous storage and sentiment analysis.\n",
    "\n",
    "Below code is a modification of what was found on https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/. This is basically a learning tool because, so far, the results are pretty shit at analysis. Suggestions moving forward: implement VADER as it is calibrated to social media data already (rather than movie reviews, like textblob). If you're feeling ambitious, get some preliminary data first, build your own damn model on what you're trying to predict, and then use that retroactively as well. \n",
    "\n",
    "\n",
    "STEPS: \n",
    "1. Build a continuous streamer to store data. Probably don't need to pay for storage if you just do it while your computer is running. Or, fuck it, pay for storage. \n",
    "2. Tag the data. This will be pretty damn painstaking. You'll really have to narrow down your tracker keywords, and make sure they get what you want, otherwise enjoy labelling MOUNTAINS of tweets that will be mostly noise. \n",
    "3. Train the model on that data. You better make sure it's predictive otherwise goodnight project.\n",
    "4. Test the data on historical data to see if it is accurate. Hopefully whatever keywords you identify haven't changed much over time. \n",
    "5. Build a trendline of the data. Hopefully it matches all of the legislation.\n",
    "6. Make forecasts for the rest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tweepy \n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob \n",
    "\n",
    "class TwitterClient(object): \n",
    "\t''' \n",
    "\tGeneric Twitter Class for sentiment analysis. \n",
    "\t'''\n",
    "\tdef __init__(self): \n",
    "\t\t''' \n",
    "\t\tClass constructor or initialization method. \n",
    "\t\t'''\n",
    "\t\t# keys and tokens from the Twitter Dev Console \n",
    "\t\tconsumer_key = 'qtIWsigy1FG4kyLIe8darX3vf'\n",
    "\t\tconsumer_secret = 'fEt7blLkFFWMppZv2cMt58PN1o7ridztG8SzoNnM5XRR8hwgqb'\n",
    "\t\taccess_token = '1507759020-g9om0JN1evu98wtSjnOwVRDK3sXLX0QdpzRohw5'\n",
    "\t\taccess_token_secret = 'lrerVx5Y2uoxCRoHf7Xhfco2NSiw5T8YKojcXISTQz4ru'\n",
    "\n",
    "\t\t# attempt authentication \n",
    "\t\ttry: \n",
    "\t\t\t# create OAuthHandler object \n",
    "\t\t\tself.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "\t\t\t# set access token and secret \n",
    "\t\t\tself.auth.set_access_token(access_token, access_token_secret) \n",
    "\t\t\t# create tweepy API object to fetch tweets \n",
    "\t\t\tself.api = tweepy.API(self.auth) \n",
    "\t\texcept: \n",
    "\t\t\tprint(\"Error: Authentication Failed\") \n",
    "\n",
    "\tdef clean_tweet(self, tweet): \n",
    "\t\t''' \n",
    "\t\tUtility function to clean tweet text by removing links, special characters \n",
    "\t\tusing simple regex statements. \n",
    "\t\t'''\n",
    "\t\treturn ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "\n",
    "\tdef get_tweet_sentiment(self, tweet): \n",
    "\t\t''' \n",
    "\t\tUtility function to classify sentiment of passed tweet \n",
    "\t\tusing textblob's sentiment method \n",
    "\t\t'''\n",
    "\t\t# create TextBlob object of passed tweet text \n",
    "\t\tanalysis = TextBlob(self.clean_tweet(tweet)) \n",
    "\t\t# set sentiment \n",
    "\t\tif analysis.sentiment.polarity > 0: \n",
    "\t\t\treturn 'positive'\n",
    "\t\telif analysis.sentiment.polarity == 0: \n",
    "\t\t\treturn 'neutral'\n",
    "\t\telse: \n",
    "\t\t\treturn 'negative'\n",
    "\n",
    "\tdef get_tweets(self, query, count = 10): \n",
    "\t\t''' \n",
    "\t\tMain function to fetch tweets and parse them. \n",
    "\t\t'''\n",
    "\t\t# empty list to store parsed tweets \n",
    "\t\ttweets = [] \n",
    "\n",
    "\t\ttry: \n",
    "\t\t\t# call twitter api to fetch tweets \n",
    "\t\t\tfetched_tweets = self.api.search(q = query, count = count) \n",
    "\n",
    "\t\t\t# parsing tweets one by one \n",
    "\t\t\tfor tweet in fetched_tweets: \n",
    "\t\t\t\t# empty dictionary to store required params of a tweet \n",
    "\t\t\t\tparsed_tweet = {} \n",
    "\n",
    "\t\t\t\t# saving text of tweet \n",
    "\t\t\t\tparsed_tweet['text'] = tweet.text \n",
    "\t\t\t\t# saving sentiment of tweet \n",
    "\t\t\t\tparsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "\n",
    "\t\t\t\t# appending parsed tweet to tweets list \n",
    "\t\t\t\tif tweet.retweet_count > 0: \n",
    "\t\t\t\t\t# if tweet has retweets, ensure that it is appended only once \n",
    "\t\t\t\t\tif parsed_tweet not in tweets: \n",
    "\t\t\t\t\t\ttweets.append(parsed_tweet) \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\ttweets.append(parsed_tweet) \n",
    "\n",
    "\t\t\t# return parsed tweets \n",
    "\t\t\treturn tweets \n",
    "\n",
    "\t\texcept tweepy.TweepError as e: \n",
    "\t\t\t# print error (if any) \n",
    "\t\t\tprint(\"Error : \" + str(e)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets percentage: 49.31506849315068 %\n",
      "Negative tweets percentage: 6.8493150684931505 %\n",
      "Neutral tweets percentage: 43.83561643835616 %\n",
      "\n",
      "\n",
      "Positive tweets:\n",
      "RT @thehowie: This is why testing/more data would have been so helpful. Risk of keeping public schools open is to elderly &amp; otherwise compr…\n",
      "RT @CornellILR_News: Research co-authored by @CornnellILR asst. prof. @iajunwa details results of the first nationally representative surve…\n",
      "Lockdown is open source, so you can see exactly what it's doing and *not* doing.\n",
      "Never trust a closed source securi… https://t.co/TC9m2kK5dj\n",
      "Spectator entrance, WIAA Boys Basketball, at Brillion High School, Xavier vs Wrightstown.\n",
      "https://t.co/KFFhu3ubek\n",
      "1… https://t.co/4cUQDSpzFb\n",
      "@rowlsmanthorpe @drdrmc @lilianedwards Open Banking isn't about that kind of openness, it's about giving customers… https://t.co/M1z4Rv5zpa\n",
      "I love seeing data that curation makes for more usable open data (as opposed to researcher self-deposit in a data repository). #RDAP20\n",
      "Great new paper from Nature, including @KeeleGGE researchers @hypocentre and Glenda Jones, on open data infrastruct… https://t.co/Iig73QNhwP\n",
      "@rowlsmanthorpe @lilianedwards Not an issue - interested in clarifying what we meant - indeed just like open bankin… https://t.co/1femTTTVOY\n",
      "Smart City Standards and Open Data - Our CTO Chris Cooper guests on The Smart Community Podcast -… https://t.co/d6vU9dUwhy\n",
      "RT @uaptn: At UAPTN, we plan on being completely transparent about the data we collect.  Our data will be made available via @grpcio for an…\n",
      "\n",
      "\n",
      "Negative tweets:\n",
      "RT @CaseyKneale: Ever wonder how you could propagate uncertainties through arbitrary model spaces? I explored this in a blog post using the…\n",
      "This is NOT OK on any level. Blocking specialists because of lack of clearance for discussions that have no reason… https://t.co/Us4uLCKi1G\n",
      "Dow Jones Industrial Average\n",
      "As of 20:44 11 Mar 2020\n",
      "\n",
      "15 min delay\n",
      "\n",
      "Source: WebFG\n",
      "\n",
      "Today's data summary\n",
      "Market open… https://t.co/YBWkxX3yJD\n",
      "RT @CBS_Herridge: DEVELOPING: Secret Service COVID-19 Phishing alert @CBSNews Cyber criminal emails pose as legitimate medical or health gr…\n",
      "Ever wonder how you could propagate uncertainties through arbitrary model spaces? I explored this in a blog post us… https://t.co/3pZ56zTv4j\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(): \n",
    "\t# creating object of TwitterClient Class \n",
    "\tapi = TwitterClient() \n",
    "\t# calling function to get tweets \n",
    "\ttweets = api.get_tweets(query = 'open data', count = 200) \n",
    "\n",
    "\t# picking positive tweets from tweets \n",
    "\tptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "\t# percentage of positive tweets \n",
    "\tprint(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "\t# picking negative tweets from tweets \n",
    "\tntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "\t# percentage of negative tweets \n",
    "\tprint(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "\t# percentage of neutral tweets \n",
    "\tprint(\"Neutral tweets percentage: {} %\".format(100*(len(tweets) - len(ntweets) - len(ptweets))/len(tweets)))\n",
    "\tprint(\"\\n\\nPositive tweets:\") \n",
    "\tfor tweet in ptweets[:10]: \n",
    "\t\tprint(tweet['text']) \n",
    "\n",
    "\t# printing first 5 negative tweets \n",
    "\tprint(\"\\n\\nNegative tweets:\") \n",
    "\tfor tweet in ntweets[:10]: \n",
    "\t\tprint(tweet['text']) \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\t# calling main function \n",
    "\tmain() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamer Data\n",
    "Look below to see some sample code from this article:https://towardsdatascience.com/mining-live-twitter-data-for-sentiment-analysis-of-events-d69aa2d136a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tweepy\n",
    "from tweepy import Stream                   # Useful in Step 3\n",
    "from tweepy.streaming import StreamListener # Useful in Step 3\n",
    "consumer_key = os.getenv(“CONSUMER_KEY_TWITTER”)\n",
    "consumer_secret = os.getenv(“CONSUMER_SECRET_TWITTER”)\n",
    "access_token = os.getenv(“ACCESS_KEY_TWITTER”)\n",
    "access_token_secret = os.getenv(“ACCESS_SECRET_TWITTER”)\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "## Defining a class Listener so that you can manipulate data.\n",
    "class listener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        data = json.loads(data)\n",
    "        # Filter out non-English Tweets\n",
    "        if data.get(\"lang\") != \"en\": \n",
    "            return True\n",
    "        try:\n",
    "            timestamp = data['timestamp_ms']            \n",
    "            # Get longer 280 char tweets if possible\n",
    "            if data.get(\"extended_tweet\"):\n",
    "                tweet = data['extended_tweet'][\"full_text\"]\n",
    "            else:\n",
    "                tweet = data[\"text\"]\n",
    "            url = \"https://www.twitter.com/i/web/status/\" +   \n",
    "                   data[\"id_str\"]\n",
    "            user = data[\"user\"][\"screen_name\"]\n",
    "            verified = data[\"user\"][\"verified\"]                  \n",
    "            write_to_csv([timestamp, tweet, user, verified, url])\n",
    "            ## include a write_to_csv statement to save this to your device. O/W setup eternal storage.\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(\"Keyerror:\", e)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "    \n",
    "## Defining a function to actually stream and write. \n",
    "#Note that we use the extended mode to capture as much info as possible. \n",
    "def stream_and_write(table, track=None):\n",
    "    try:\n",
    "        twitterStream = Stream(auth, listener(), \n",
    "                               tweet_mode='extended')\n",
    "        twitterStream.filter(track=[\"AAPL\", \"AMZN\", \"UBER\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))\n",
    "        time.sleep(5)\n",
    "        \n",
    "        \n",
    "## Here's the vader model. \n",
    "#You can probably check to see if it is getting good results over time, rather than spot checking. Probably good to spot check at first though.\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vs = analyzer.polarity_scores(tweet)\n",
    "print(vs[\"compound\"], vs[\"pos\"], vs[\"neu\"], vs[\"neg\"])\n",
    "\n",
    "\n",
    "## if You don't get good results with Vader. Then use the data you've stored to build and test your own model. \n",
    "#IF, and that's a big if, it performs better than VADER, then use it. Bonus points for building your own shit, I mean the longest part is gonna be collecting enough usuable data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Data\n",
    "Unfortunately, Tweepy's library only supports basic search functions, making it suitable for 7-day - real-time updates. To look at historical data, we can use Twitter's own in-house library, Twitter (see https://twitterdev.github.io/search-tweets-python/ for documentation.) The majority of the retrospective analysis will be done doing this. Key question we seek to answer: - What were the trends that led to GDPR & CCPA being adopted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
